{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: This notebook is used in the following four videos:\n",
    "\n",
    "[**Work with missing data in a Python notebook**](https://www.coursera.org/teach/go-beyond-the-numbers-translate-data-into-insight/mjPtRyfIEe2GwRLfSI7mvQ/content/item/lecture/rUXcJ/video-subtitles) ðŸ š [jump to notebook section](#work_with_missing_data)\n",
    "\n",
    "[**Identify and deal with outliers in Python**](https://www.coursera.org/teach/go-beyond-the-numbers-translate-data-into-insight/mjPtRyfIEe2GwRLfSI7mvQ/content/item/lecture/jadID/video-subtitles) ðŸ š [jump to notebook section](#outliers)  \n",
    "\n",
    "[**Label encoding in Python**](https://www.coursera.org/teach/go-beyond-the-numbers-translate-data-into-insight/mjPtRyfIEe2GwRLfSI7mvQ/content/item/lecture/fLMxl/video-subtitles) ðŸ š [jump to notebook section](#encoding) \n",
    "\n",
    "[**Input validation with Python**](https://www.coursera.org/teach/go-beyond-the-numbers-translate-data-into-insight/mjPtRyfIEe2GwRLfSI7mvQ/content/item/lecture/C6Mok/video-subtitles) ðŸ š [jump to notebook section](#input_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lqaIRwaHnHN"
   },
   "source": [
    "<a id='work_with_missing_data'></a>\n",
    "# Work with missing data in a Python notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GC1ihMO9Mx3z"
   },
   "source": [
    "Throughout the following exercises, you will be discovering and working with missing data on a dataset.  Before starting on this programming exercise, we strongly recommend watching the video lecture and completing the IVQ for the associated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNQAhGu9HNNA"
   },
   "source": [
    "All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB8sXqMYHNNA"
   },
   "source": [
    "As we move forward, you can find instructions on how to install required libraries as they arise in this notebook. Before we begin with the exercises and analyzing the data, we need to import all libraries and extensions required for this programming exercise. Throughout the course, we will be using pandas, numpy, datetime, for operations, and matplotlib, pyplot and seaborn for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "We will be examining lightning strike data collected by the National Oceanic and Atmospheric Association (NOAA) for the month of August 2018. There are two datasets. The first includes five columns:  \n",
    "\n",
    "|date|center_point_geom|longitude|latitude|number_of_strikes|\n",
    "|---|---|---|---|---|\n",
    "\n",
    "The second dataset contains seven columns:\n",
    "\n",
    "|date|zip_code|city|state|state_code|center_point_geom|number_of_strikes|\n",
    "|---|---|---|---|---|---|---|  \n",
    "\n",
    "The first dataset has two unique colums: `longitude` and `latitude`.  \n",
    "The second dataset has four unique columns: `zip_code`, `city`, `state`, and `state_code`.  \n",
    "There are three columns that are common between them: `date`, `center_point_geom`, and `number_of_strikes`.\n",
    "\n",
    "We want to combine the two datasets into a single dataframe that has all of the information from both datasets. Ideally, both datasets will have the same number of entries for the same locations on the same dates. If they don't, we'll investigate which data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1666854251438,
     "user": {
      "displayName": "Jim McCoy",
      "userId": "05540602321492626965"
     },
     "user_tz": 300
    },
    "id": "2406K3ewQavA"
   },
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = '/home/gato/Scripts/DS/DataIntoInsights/data/Data_week_3/\\\n",
    "eda_missing_data_dataset1.csv'\n",
    "path2 = '/home/gato/Scripts/DS/DataIntoInsights/data/Data_week_3/\\\n",
    "eda_missing_data_dataset2.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "executionInfo": {
     "elapsed": 538,
     "status": "error",
     "timestamp": 1666854253863,
     "user": {
      "displayName": "Jim McCoy",
      "userId": "05540602321492626965"
     },
     "user_tz": 300
    },
    "id": "XqpO1o0FHNNC",
    "outputId": "8ce21350-23ae-4f21-c2e6-4e7833b44950"
   },
   "outputs": [],
   "source": [
    "# Read in first dataset\n",
    "df = pd.read_csv(path1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-CW5tG0HNNE"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows of dataset 1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqiuJyKOHNNE"
   },
   "source": [
    "Let's check on our dataset shape to determine number of columns and rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6bdTmL21ABQ_",
    "outputId": "a23f6759-00b1-483f-e1e0-b78c0af9f8a0"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-jlpqMTHNNF"
   },
   "source": [
    "Now we'll read in the second dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gh3Tc0gy7akq"
   },
   "outputs": [],
   "source": [
    "# Read in second dataset\n",
    "df_zip = pd.read_csv(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "id": "hXcTIHM39ZgS",
    "outputId": "d330ddda-cb58-430f-d65b-6e5a35239139"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows of dataset 2\n",
    "df_zip.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the shape..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHZ6mXMuHNNG"
   },
   "outputs": [],
   "source": [
    "df_zip.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBE8enzyAV14",
    "outputId": "c2913d76-8092-47d1-b143-f92304e10c76"
   },
   "source": [
    "Hmmm... This dataset has less than half the number of rows as the first one. But which ones are they?  \n",
    "\n",
    "The first thing we'll do to explore this discrepancy is join the two datasets into a single dataframe. We can do this using the `merge()` method of the `DataFrame` class. For more information about the `merge()` method, refer to the [merge() pandas documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html).  \n",
    "\n",
    "Begin with the first dataframe (`df`) and call the `merge()` method on it. The first argument is a positional argument that specifies the dataframe we want to merge with, known as the `right` dataframe. (The dataframe you're calling the method on is always the `left` dataframe.) The `how` argument specifies which dataframe's keys we'll use to match to, and the `on` argument lets us define the columns to use as keys. \n",
    "\n",
    "A demonstration will make this easiest to understand. Refer to the **[BONUS CONTENT](#merge_bonus)** at the end of the notebook for different examples of the `merge()` method.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u1KobuXtHNNH"
   },
   "outputs": [],
   "source": [
    "# Left-join the two datasets\n",
    "df_joined = df.merge(df_zip, how='left', on=['date','center_point_geom'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "oFqcI8EWz5zo",
    "outputId": "89c63b78-92f3-48aa-a94c-15a5ca9d61a0"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows of the merged data\n",
    "df_joined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCNnmVpcHNNH"
   },
   "source": [
    "Notice that the new dataframe has all of the columns of both original dataframes, and it has two `number_of_strikes` columns that are suffixed with `_x` and `_y`. This is because the key columns from both dataframes were the same, so they appear once in the merged dataframe. The unique columns of each original dataframe also appear in the merged dataframe. But both original dataframes had another column&mdash;`number_of_strikes`&mdash;that had the same name in both dataframes and was not indicated as a key. Pandas handles this by adding both columns to the new dataframe. \n",
    "\n",
    "Now we'll check the summary on this joined dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "7hO9B0MC-GBU",
    "outputId": "8c960db3-250c-4406-bd8b-8e442e328c1d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get descriptive statistics of the joined dataframe\n",
    "df_joined.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DuKB8VNHNNI"
   },
   "source": [
    "The count information confirms that the new dataframe is missing some data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqViIoaoP-lZ",
    "outputId": "b0e6d193-7156-4a6e-c7b5-b5e29a10e7ab"
   },
   "source": [
    "Now let's check how many missing state locations we have by using `isnull()` to create a Boolean mask that we'll apply to `df_joined`. The mask is a pandas `Series` object that contains `True` for every row with a missing `state_code` value and `False` for every row that is not missing data in this column. When the mask is applied to `df_joined`, it filters out the rows that are not missing `state_code` data. (Note that using the `state_code` column to create this mask is an arbitrary decision. We could have selected `zip_code`, `city`, or `state` instead and gotten the same results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkKLc4cyHNNI"
   },
   "outputs": [],
   "source": [
    "# Create a new df of just the rows that are missing data\n",
    "df_null_geo = df_joined[pd.isnull(df_joined.state_code)]\n",
    "df_null_geo.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can confirm that `df_null_geo` contains only the rows with the missing `state_code` values by using the `info()` method on `df_joined` and comparing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PsQMbNgQbRq",
    "outputId": "fa24068c-94ab-4aef-89ed-a0a974755511",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get non-null counts on merged dataframe\n",
    "df_joined.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we subtract the 323,700 non-null rows in columns 5-9 of `df_joined` from the 717,530 non-null rows in columns 0-4 of `df_joined`, we're left with 393,830 rows that contain missing data&mdash;the same number of rows contained in `df_null_geo`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "odPP5Ctf-JUh",
    "outputId": "68bb4431-71a1-4c3a-d986-49c494bd9704"
   },
   "outputs": [],
   "source": [
    "# Print the first 5 rows\n",
    "df_null_geo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jemHAMYQ9sJ",
    "outputId": "a2360cd0-139f-4a7d-a9cc-c87fb00f96d3"
   },
   "source": [
    "Now that we've merged all of our data together and isolated the rows with missing data, we can better understand what data is missing by plotting the longitude and latitude of locations that are missing city, state, and zip code data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxgebm1cHNNJ"
   },
   "outputs": [],
   "source": [
    "# Create new df of just latitude, longitude, and number of strikes and group by latitude and longitude\n",
    "top_missing = df_null_geo[['latitude','longitude','number_of_strikes_x']\n",
    "            ].groupby(['latitude','longitude']\n",
    "                      ).sum().sort_values('number_of_strikes_x',ascending=False).reset_index()\n",
    "top_missing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmr_w0wWHNNJ"
   },
   "source": [
    "Let's import plotly to reduce the size of the data frame as we create a geographic scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "cHfTn138zksI",
    "outputId": "482c49dd-b16b-482d-a6b5-9170803375fa"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px  # Be sure to import express\n",
    "# reduce size of db otherwise it could break\n",
    "fig = px.scatter_geo(top_missing[top_missing.number_of_strikes_x>=300],  # Input Pandas DataFrame\n",
    "                    lat=\"latitude\",  # DataFrame column with latitude\n",
    "                    lon=\"longitude\",  # DataFrame column with latitude\n",
    "                    size=\"number_of_strikes_x\") # Set to plot size as number of strikes\n",
    "fig.update_layout(\n",
    "    title_text = 'Missing data', # Create a Title\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NguRaQFiHNNK"
   },
   "source": [
    "Itâ€™s a nice geographic visualization, but we really donâ€™t need the global scale. Letâ€™s scale it down to only the geographic area we are interested in - the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "XL35TotjXuBD",
    "outputId": "449a50fe-205b-49e6-bb0d-2fb1c6026dcc"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px  # Be sure to import express\n",
    "fig = px.scatter_geo(top_missing[top_missing.number_of_strikes_x>=300],  # Input Pandas DataFrame\n",
    "                    lat=\"latitude\",  # DataFrame column with latitude\n",
    "                    lon=\"longitude\",  # DataFrame column with latitude\n",
    "                    size=\"number_of_strikes_x\") # Set to plot size as number of strikes\n",
    "fig.update_layout(\n",
    "    title_text = 'Missing data', # Create a Title\n",
    "    geo_scope='usa',  # Plot only the USA instead of globe\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bsggl8Sv4R9Z"
   },
   "source": [
    "This explains why so many rows were missing state and zip code data! Most of these lightning strikes occurred over water&mdash;the Atlantic Ocean, the Sea of Cortez, the Gulf of Mexico, the Caribbean Sea, and the Great Lakes. Of the strikes that occurred over land, most of those were in Mexico, the Bahamas, and Cuba&mdash;places outside of the U.S. and without U.S. zip codes. Nonetheless, some of the missing data is from Florida and elsewhere within the United States, and we might want to ask the database owner about this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1bs5m4TIDve"
   },
   "source": [
    "If you have successfully completed the material above, congratulations! You now understand handling missing data in Python and should be able to start using it on your own datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='merge_bonus'></a>\n",
    "### Bonus (not in video): `df.merge()` demonstration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with two dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define df1\n",
    "data = {'planet': ['Mercury', 'Venus', 'Earth', 'Mars',\n",
    "                    'Jupiter', 'Saturn', 'Uranus', 'Neptune'],\n",
    "        'radius_km': [2440, 6052, 6371, 3390, 69911, 58232,\n",
    "                      25362, 24622],\n",
    "        'moons': [0, 0, 1, 2, 80, 83, 27, 14]\n",
    "         }\n",
    "df1 = pd.DataFrame(data)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define df2\n",
    "data = {'planet': ['Mercury', 'Venus', 'Earth', 'Meztli', 'Janssen'],\n",
    "        'radius_km': [2440, 6052, 6371, 48654, 11959],\n",
    "        'life?': ['no', 'no', 'yes', 'no', 'yes'],\n",
    "         }\n",
    "df2 = pd.DataFrame(data)\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll merge the two dataframes on the `['planet', 'radius_km]` columns. Try running the below cell with each of the following arguments for the **`how`** keyword: `'left'`, `'right'`, `'inner'`, and `'outer'`. Notice how each argument changes the result.  \n",
    "\n",
    "Feel free to change the columns specified by the **`on`** argument too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = df1.merge(df2, how='left', on=['planet', 'radius_km'])\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CbSkUElG8U6y"
   },
   "source": [
    "<a id='outliers'></a>\n",
    "# Identify and deal with outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDYdUUfu8U63"
   },
   "source": [
    "Throughout the following exercises, you will learn to find and deal with outliers in a dataset. Before starting on this programming exercise, we strongly recommend watching the video lecture and completing the IVQ for the associated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA-_MCnQ8U63"
   },
   "source": [
    "All the information you need for solving this assignment is in this notebook, and all the code you will be implementing will take place within this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeLx9HXZ8U64"
   },
   "source": [
    "As we move forward, you can find instructions on how to install required libraries as they arise in this notebook. Before we begin with the exercises and analyzing the data, we need to import all libraries and extensions required for this programming exercise. Throughout the course, we will be using pandas, numpy, datetime, for operations, and matplotlib, pyplot and seaborn for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "We will be examining lightning strike data collected by the National Oceanic and Atmospheric Association (NOAA) from 1987 through 2020. Because this would be many millions of rows to read into the notebook, we've preprocessed the data so it contains just the year and the number of strikes.\n",
    "\n",
    "We will examine the range of total lightning strike counts for each year and identify outliers. Then we will plot the yearly totals on a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1TrZmUX1-ruY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "klv0Mx6Tx8f0"
   },
   "outputs": [],
   "source": [
    "# Read in data\n",
    "df = pd.read_csv('eda_outliers_dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "LGFoXjaK-xwl",
    "outputId": "12fdc439-a34a-48eb-9dfb-32b0c479d225"
   },
   "outputs": [],
   "source": [
    "# Print first 10 rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZlwVqzq8U68"
   },
   "source": [
    "Next, let's convert the number of strikes value to a more readable format on the graph (e.g., converting 100,000 to 100K, 3,000,000 to 3M, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRdByot6-yC3"
   },
   "outputs": [],
   "source": [
    "def readable_numbers(x):\n",
    "    \"\"\"takes a large number and formats it into K,M to make it more readable\"\"\"\n",
    "    if x >= 1e6:\n",
    "        s = '{:1.1f}M'.format(x*1e-6)\n",
    "    else:\n",
    "        s = '{:1.0f}K'.format(x*1e-3)\n",
    "    return s\n",
    "\n",
    "# Use the readable_numbers() function to create a new column \n",
    "df['number_of_strikes_readable']=df['number_of_strikes'].apply(readable_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "XEXT91XkKAGM",
    "outputId": "faf02df8-e22a-4ab8-ef7b-a306182eaa21"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbXU1Mi5-17d",
    "outputId": "3801d785-79e9-456b-b379-2777641b42cd"
   },
   "outputs": [],
   "source": [
    "print(\"Mean:\" + readable_numbers(np.mean(df['number_of_strikes'])))\n",
    "print(\"Median:\" + readable_numbers(np.median(df['number_of_strikes'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-P4nzaEG8U6_"
   },
   "source": [
    "A boxplot can help to visually break down the data into percentiles / quartiles, which are important summary statistics. The shaded center of the box represents the middle 50th percentile of the data points. This is the interquartile range, or IQR. \n",
    "\n",
    "The boxplot \"whiskers\" extend 1.5x the IQR by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boxplot\n",
    "box = sns.boxplot(x=df['number_of_strikes'])\n",
    "g = plt.gca()\n",
    "box.set_xticklabels(np.array([readable_numbers(x) for x in g.get_xticks()]))\n",
    "plt.xlabel('Number of strikes')\n",
    "plt.title('Yearly number of lightning strikes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5Uiz_im8U7A"
   },
   "source": [
    "The points to the left of the left whisker are outliers. Any observations that are more than 1.5 IQR below Q1 or more than 1.5 IQR above Q3 are considered outliers.\n",
    "\n",
    "One important point for every data professional: do not assume an outlier is erroneous unless there is an explanation or reason to do so.\n",
    "\n",
    "Let's define our IQR, upper, and lower limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9FPmmWVn-5Mu",
    "outputId": "3e2272bf-61b7-4142-a1c2-fa527263a877",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate 25th percentile of annual strikes\n",
    "percentile25 = df['number_of_strikes'].quantile(0.25)\n",
    "\n",
    "# Calculate 75th percentile of annual strikes\n",
    "percentile75 = df['number_of_strikes'].quantile(0.75)\n",
    "\n",
    "# Calculate interquartile range\n",
    "iqr = percentile75 - percentile25\n",
    "\n",
    "# Calculate upper and lower thresholds for outliers\n",
    "upper_limit = percentile75 + 1.5 * iqr\n",
    "lower_limit = percentile25 - 1.5 * iqr\n",
    "\n",
    "print('Lower limit is: '+ readable_numbers(lower_limit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use a Boolean mask to select only the rows of the dataframe where the number of strikes is less than the lower limit we calculated above. These rows are the outliers on the low end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "KnhusKPV-6-C",
    "outputId": "4d95bdc6-8236-4642-d72f-c647c67c27d6"
   },
   "outputs": [],
   "source": [
    "# Isolate outliers on low end\n",
    "df[df['number_of_strikes'] < lower_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xoO1lvra8U7B"
   },
   "source": [
    "Let's get a visual of all of the data points with the outlier values colored red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "IPX0t_Be-_zD",
    "outputId": "d06c9533-4b69-474a-cd23-788ba72a90d7"
   },
   "outputs": [],
   "source": [
    "def addlabels(x,y):\n",
    "    for i in range(len(x)):\n",
    "        plt.text(x[i]-0.5, y[i]+500000, s=readable_numbers(y[i]))\n",
    "\n",
    "colors = np.where(df['number_of_strikes'] < lower_limit, 'r', 'b')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.scatter(df['year'], df['number_of_strikes'],c=colors)\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Number of strikes')\n",
    "ax.set_title('Number of lightning strikes by year')\n",
    "addlabels(df['year'], df['number_of_strikes'])\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6a8Bf_8G_Gjt"
   },
   "source": [
    "### Investigating the outliers 2019 and 1987\n",
    "\n",
    "Let's examine the two outlier years a bit more closely. In the section above, we used a preprocessed dataset that didn't include a lot of the information that we're accustomed to having in this data. In order to further investigate the outlier years, we'll need more information, so we're going to import data from these years specifically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_Jmq8rn8U7C"
   },
   "source": [
    "#### Import data for 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3zF2zgB5hWo"
   },
   "outputs": [],
   "source": [
    "df_2019 = pd.read_csv('eda_outliers_dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll convert the `date` column to datetime. This will enable us to extract two new columns: `month` and `month_txt`. Then, we'll sort the data by `month` and `month_txt`, sum it, and sort the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "wCtWh9mK_D6v",
    "outputId": "9513bf70-2919-424b-9eae-9f10ef98e3cd"
   },
   "outputs": [],
   "source": [
    "# Convert `date` column to datetime\n",
    "df_2019['date']= pd.to_datetime(df_2019['date'])\n",
    "\n",
    "# Create 2 new columns\n",
    "df_2019['month'] = df_2019['date'].dt.month\n",
    "df_2019['month_txt'] = df_2019['date'].dt.month_name().str.slice(stop=3)\n",
    "\n",
    "# Group by `month` and `month_txt`, sum it, and sort. Assign result to new df\n",
    "df_2019_by_month = df_2019.groupby(['month','month_txt']).sum().sort_values('month', ascending=True).head(12).reset_index()\n",
    "df_2019_by_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8dHUg74_F4d"
   },
   "source": [
    "2019 appears to have data only for the month of December. The likelihood of there not being any lightning from January to November 2019 is ~0. This appears to be a case of missing data. We should probably exclude 2019 from the analysis (for most use cases)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URIJFvLy_Owp"
   },
   "source": [
    "#### Import data for 1987\n",
    "\n",
    "Now let's inspect the data from the other outlier year, 1987.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qe9Res5JUNN"
   },
   "outputs": [],
   "source": [
    "# Read in 1987 data\n",
    "df_1987 = pd.read_csv('eda_outliers_dataset3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMWnr-ZB8U7D"
   },
   "source": [
    "In this code block we will do the same datetime conversions and groupings we did for the other datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "eAYJLFUv_RjN",
    "outputId": "cb6ba40e-9ef0-4078-8742-32b3d265339e"
   },
   "outputs": [],
   "source": [
    "# Convert `date` column to datetime\n",
    "df_1987['date'] = pd.to_datetime(df_1987['date'])\n",
    "\n",
    "# Create 2 new columns\n",
    "df_1987['month'] = df_1987['date'].dt.month\n",
    "df_1987['month_txt'] = df_1987['date'].dt.month_name().str.slice(stop=3)\n",
    "\n",
    "# Group by `month` and `month_txt`, sum it, and sort. Assign result to new df\n",
    "df_1987_by_month = df_1987.groupby(['month','month_txt']).sum().sort_values('month', ascending=True).head(12).reset_index()\n",
    "df_1987_by_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvQlQBDI8U7D"
   },
   "source": [
    "1987 has data for every month of the year. Hence, this outlier should be treated differently than 2019, which is missing data. \n",
    "\n",
    "Finally, let's re-run the mean and median after removing the outliers. Our final takeaway from our lesson on outliers is that outliers significantly affect the dataset's mean, but do not significantly affect the median. \n",
    "\n",
    "To remove the outliers, we'll use a Boolean mask to create a new dataframe that contains only the rows in the original dataframe where the number of strikes >= the lower limit we calculated above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sMRj1qXj_TH3",
    "outputId": "f15a3a6c-2c4e-4b97-d04e-4181e55b02d9"
   },
   "outputs": [],
   "source": [
    "# Create new df that removes outliers\n",
    "df_without_outliers = df[df['number_of_strikes'] >= lower_limit]\n",
    "\n",
    "# Recalculate mean and median values on data without outliers\n",
    "print(\"Mean:\" + readable_numbers(np.mean(df_without_outliers['number_of_strikes'])))\n",
    "print(\"Median:\" + readable_numbers(np.median(df_without_outliers['number_of_strikes'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the mean and the median changed, but the mean much more so. It is clear that outlier values can affect the distributions of the data and the conclusions that can be drawn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwZ51SjQ8U7E"
   },
   "source": [
    "Both the mean and the median changed, but the mean much more so. It is clear that outlier values can affect the distributions of the data and the conclusions that can be drawn from them.\n",
    "If you have successfully completed the material above, congratulations! You now understand discovering in Python and should be able to start using it on your own datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov5UIve3_Ii7"
   },
   "source": [
    "<a id='encoding'></a>\n",
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKvCNpDJAbjG"
   },
   "source": [
    "Throughout the following exercises, you will practice label encoding in Python.  Before starting on this programming exercise, we strongly recommend watching the video lecture and completing the IVQ for the associated topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36P-tGhtAbjH"
   },
   "source": [
    "As we move forward, you can find instructions on how to install required libraries as they arise in this notebook. Before we begin with the exercises and analyzing the data, we need to import all libraries and extensions required for this programming exercise. Throughout the course, we will be using pandas for operations, and matplotlib and seaborn for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "We will be examining monthly lightning strike data collected by the National Oceanic and Atmospheric Association (NOAA) for 2016&ndash;2018. The dataset includes three columns:  \n",
    "\n",
    "|date|number_of_strikes|center_point_geom|\n",
    "|---|---|---|  \n",
    "\n",
    "The objective is to assign the monthly number of strikes to the following categories: mild, scattered, heavy, or severe. Then we will create a heatmap of the three years so we can get a high-level understanding of monthly lightning severity from a simple diagram.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p3iejYYsqe4a"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "path = '/home/gato/Scripts/DS/DataIntoInsights/data/Data_week_3/eda_label_encoding_dataset.csv'\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80TUmVhX_5Pa"
   },
   "source": [
    "### Create a categorical variable `strike_level`\n",
    "\n",
    "Begin by converting the `date` column to datetime. Then we'll create a new `month` column that contains the first three letters of each month.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EiBqYMXyAbjJ"
   },
   "outputs": [],
   "source": [
    "# Convert `date` column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create new `month` column\n",
    "df['month'] = df['date'].dt.month_name().str.slice(stop=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1HlVZ8YAbjK",
    "outputId": "7d3741c1-3469-48fa-bffd-866e8e19b1c0"
   },
   "source": [
    "Next, we'll encode the months as categorical information. This allows us to specifically designate them as categories that adhere to a specific order, which is helpful when we plot them later. We'll also create a new `year` column. Then we'll group the data by year and month, sum the remaining columns, and assign the results to a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "35JhGi3ZAbjK",
    "outputId": "7d3741c1-3469-48fa-bffd-866e8e19b1c0"
   },
   "outputs": [],
   "source": [
    "# Create categorical designations\n",
    "months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Encode `month` column as categoricals \n",
    "df['month'] = pd.Categorical(df['month'], categories=months, ordered=True)\n",
    "\n",
    "# Create `year` column by extracting the year info from the datetime object\n",
    "df['year'] = df['date'].dt.strftime('%Y')\n",
    "\n",
    "# Create a new df of month, year, total strikes\n",
    "df_by_month = df.groupby(['year', 'month']).sum().reset_index()\n",
    "df_by_month.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3sFkl-6shOn",
    "outputId": "f11949b8-f2a2-4ec2-ffa0-a3f99a79c4e5"
   },
   "source": [
    "Now we'll create a new column called `strike_level` that contains a categorical variable representing the lightning strikes for each month as mild, scattered, heavy, or severe. The `pd.qcut` pandas function makes this easy. We just input the column to be categorized, the number of quantiles to sort the data into, and how we want to name each quantile. For more information on this function, refer to the [pandas qcut() documentation](https://pandas.pydata.org/docs/reference/api/pandas.qcut.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "bTVvdgaqAbjM",
    "outputId": "f11949b8-f2a2-4ec2-ffa0-a3f99a79c4e5"
   },
   "outputs": [],
   "source": [
    "# Create a new column that categorizes number_of_strikes into 1 of 4 categories\n",
    "df_by_month['strike_level'] = pd.qcut(\n",
    "    df_by_month['number_of_strikes'],\n",
    "    4,\n",
    "    labels = ['Mild', 'Scattered', 'Heavy', 'Severe'])\n",
    "df_by_month.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jm_vfDmp_-AW"
   },
   "source": [
    "### Encode `strike_level` into numerical values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1fVkW__4GME",
    "outputId": "cbff77b9-289d-4d2d-9e5a-f6b6ebec36ee"
   },
   "source": [
    "Now that we have a categorical `strike_level` column, we can extract a numerical code from it using `.cat.codes` and assign this number to a new column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "HLDyyrxlAbjN",
    "outputId": "cbff77b9-289d-4d2d-9e5a-f6b6ebec36ee"
   },
   "outputs": [],
   "source": [
    "# Create new column representing numerical value of strike level\n",
    "df_by_month['strike_level_code'] = df_by_month['strike_level'].cat.codes\n",
    "df_by_month.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdwNSo07_O3b",
    "outputId": "d199449e-e3f4-46d0-8c4f-f6f49163ef7f"
   },
   "source": [
    "We can also create binary \"dummy\" variables from the `strike_level` column. This is a useful tool if we'd like to pass the categorical variable into a model. To do this, we could use the function `pd.get_dummies()`. Note that this is just to demonstrate the functionality of `pd.get_dummies()`. Simply calling the function as we do below will not convert the data unless we reassigned the result back to a dataframe. \n",
    "\n",
    "`pd.get_dummies(df['column'])` ðŸ š **df unchanged**  \n",
    "`df = pd.get_dummies(df['column'])` ðŸ š **df changed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "46XekGd7AbjO",
    "outputId": "d199449e-e3f4-46d0-8c4f-f6f49163ef7f"
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df_by_month['strike_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to create dummy variables for our heatmap, so let's continue without converting the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZ6jf0_XAMa6"
   },
   "source": [
    "### Create a heatmap of number of strikes per month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4x-R1CKcl9o",
    "outputId": "c79dd3b1-c2cb-40e2-bf8c-1d9a83305df9"
   },
   "source": [
    "We want our heatmap to have the months on the x-axis and the years on the y-axis, and the color gradient should represent the severity (mild, scattered, heavy, severe) of lightning for each month. A simple way of preparing the data for the heatmap is to pivot it so the rows are years, columns are months, and the values are the numeric code of the lightning severity. \n",
    "\n",
    "We can do this with the `df.pivot()` method. It accepts arguments for `index`, `columns`, and `values`, which we'll specify as described. For more information on the `df.pivot()` method, refer to the [pandas pivot() method documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "eLoG-wOiAbjP",
    "outputId": "c79dd3b1-c2cb-40e2-bf8c-1d9a83305df9"
   },
   "outputs": [],
   "source": [
    "# Create new df that pivots the data\n",
    "df_by_month_plot = df_by_month.pivot(index='year', columns='month', values='strike_level_code')\n",
    "df_by_month_plot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zspYZMEQ8Tpr",
    "outputId": "080430e2-7e6a-450c-d1bb-526836b17918"
   },
   "source": [
    "At last we can plot the heatmap! We'll use seaborn's `heatmap()` function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZxXGDFIlAbjP",
    "outputId": "080430e2-7e6a-450c-d1bb-526836b17918"
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(df_by_month_plot, cmap = 'Blues')\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.set_ticks([0, 1, 2, 3])\n",
    "colorbar.set_ticklabels(['Mild', 'Scattered', 'Heavy', 'Severe'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap indicates that for all three years, the most lightning strikes occurred during the summer months. A heatmap is an easily digestable way to understand a lot of data in a single graphic.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwiQDI_kAbjQ"
   },
   "source": [
    "If you have successfully completed the material above, congratulations! You now understand how to perform label encoding in Python and should be able to start using these skills on your own datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0HJqo6-Lg7T"
   },
   "source": [
    "<a id='input_validation'></a>\n",
    "# Input Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lZOz3Nv4Lg7X"
   },
   "source": [
    "Throughout the following exercises, you will be practicing input validation in Python. Before starting on this programming exercise, we strongly recommend watching the video lecture and completing the IVQ for the associated topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JlI1p4aiLg7Y"
   },
   "source": [
    "As we move forward, you can find instructions on how to install required libraries as they arise in this notebook. Before we begin with the exercises and analyzing the data, we need to import all libraries and extensions required for this programming exercise. Throughout the course, we will be using pandas for operations, and matplotlib and seaborn for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "\n",
    "We will be examining monthly lightning strike data collected by the National Oceanic and Atmospheric Association (NOAA) for 2018. The dataset includes five columns:  \n",
    "\n",
    "|date|number_of_strikes|center_point_geom|longitude|latitude|\n",
    "|---|---|---|---|---|  \n",
    "\n",
    "The objective is to inspect the data and validate the quality of its contents. We will check for:\n",
    "  \n",
    "* Null values\n",
    "* Missing dates\n",
    "* A plausible range of daily lightning strikes in a location\n",
    "* A geographical range that aligns with expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtrtUHVuLg7Y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eBRMGvTyLg7Z"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/gato/Scripts/DS/DataIntoInsights/data/Data_week_3/eda_input_validation_joining_dataset1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dlBjGRcLg7a"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_FxCUr1Lg7b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display the data types of the columns\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eA3yzF4eLg7c"
   },
   "source": [
    "The `date` column is currently a string. Let's parse it into a datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cJKEZ_gLg7c"
   },
   "outputs": [],
   "source": [
    "# Convert `date` column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUtQ5AyMLg7c"
   },
   "source": [
    "Now we'll do some data validation. We begin by counting the number of missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PNWve412Lg7c"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNymhaqqLg7d"
   },
   "source": [
    "Check ranges for all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z53wXyTxLg7d"
   },
   "outputs": [],
   "source": [
    "df.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the number of unique dates in the `date` column is 357. This means that eight days of 2018 are missing from the data, because 2018 had 365 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYvRzvWyLg7d"
   },
   "source": [
    "### Validate `date` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIOqjRtiLg7d"
   },
   "source": [
    "We need a way to easily determine which dates are missing. We can do this by comparing all of the actual dates in 2018 to the dates we have in our `date` column. The function `pd.date_range()` will create a datetime index of all dates between a start and end date (inclusive) that we'll give as arguments. This is a very useful function that can be used for more than just days. For more information about `pd.date_range()`, refer to the [pandas date_range() function documentation](https://pandas.pydata.org/docs/reference/api/pandas.date_range.html). \n",
    "\n",
    "Once we have the datetime index object of all dates in 2018, we'll compare its contents to the dates we have in the `date` column. The `index.difference()` method is used on index objects. Its argument is an index or array that you want to compare with the one the method is being applied to. It returns the set difference of the two indices&mdash;the values that are in the original index but not in the one given in the argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zdECEMrELg7e"
   },
   "outputs": [],
   "source": [
    "# Create datetime index of every date in 2018\n",
    "full_date_range = pd.date_range(start='2018-01-01', end='2018-12-31')\n",
    "\n",
    "# Determine which values are in `full_date_range` but not in `df['date']`\n",
    "full_date_range.difference(df['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We knew that the data was missing eight dates, but now we know which specific dates they are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6G4wpdALg7e"
   },
   "source": [
    "### Validate `number_of_strikes` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q3ii4ex2Lg7e"
   },
   "source": [
    "Let's make a boxplot to better understand the range of values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLodsnIeLg7e"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y = df['number_of_strikes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a very useful visualization because the box of the interquartile range is squished at the very bottom. This is because the upper outliers are taking up all the space. Let's do it again, only this time we'll set `showfliers=False` so outliers are not included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcVyCiaELg7f"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(y = df['number_of_strikes'], showfliers=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! The interquartile range is approximately 2&ndash;12 strikes. But we know from the previous boxplot that there are many outlier days that have hundreds or even thousands of strikes. This exercise just helped us make sure that most of the dates in our data had plausible values for number of strikes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XT-53UY5Lg7f"
   },
   "source": [
    "### Validate `latitude` and `longitude` columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll create a scatterplot of all the geographical coordinates that had lightning strikes in 2018. We'll plot the points on a map to make sure the points in the data are relevant and not in unexpected locations. Because this can be a computationally intensive process, we'll prevent redundant computation by dropping rows that have the same values in their `latitude` and `longitude` columns. We can do this because the purpose here is to examine locations that had lightning strikes, but it doesn't matter how many strikes they had or when."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OGJoZMsLg7f"
   },
   "outputs": [],
   "source": [
    "# Create new df only of unique latitude and longitude combinations\n",
    "df_points = df[['latitude', 'longitude']].drop_duplicates() \n",
    "df_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = px.scatter_geo(df_points, lat = 'latitude', lon = 'longitude')\n",
    "p.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot indicates that the lightning strikes occurred primarily in the United States, but there were also many strikes in southern Canada, Mexico, and the Caribbean. We can click and move the map, and also zoom in for better resolution of the strike points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have successfully completed the material above, congratulations! You now have a better understanding of different ways to examine a dataset and validate the quality of its contents."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1ZDx8eHxRMXFHB1ol2Oi39iGPubxEdh2r",
     "timestamp": 1662733352677
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
